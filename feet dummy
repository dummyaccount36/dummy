RESULTS_DIR = "results"
os.makedirs(RESULTS_DIR, exist_ok=True)

print(f"[LR | {FEATURE_NAME}] Forecasting started at:",
      pd.to_datetime("now").strftime("%Y-%m-%d %H:%M:%S"))

print(f"[LR | {FEATURE_NAME}] Forecasting finished at:",
      pd.to_datetime("now").strftime("%Y-%m-%d %H:%M:%S"))


train_xy = pd.concat([X_train, y_train], axis=1).dropna()
X_train = train_xy.drop("so_nw_ct", axis=1)
y_train = train_xy["so_nw_ct"]



# base train_df in Polars (same as your Block M before zone split)
train_df_base = so_fcst_df[[ "zone", "key", "periods", "so_nw_ct" ]].with_columns(
    pl.col("so_nw_ct").shift(3).alias("m-0"),
    pl.col("so_nw_ct").shift(4).alias("m-1"),
    pl.col("so_nw_ct").shift(5).alias("m-2"),
).filter(pl.col("periods") >= "2023 01").with_columns(
    ma3=(pl.col("m-0")+pl.col("m-1")+pl.col("m-2"))/3
)



# Feature set 1: baseline (what you used)
train_feat_baseline = train_df_base

# Feature set 2: add extra lags + ma6
train_feat_lags_ma6 = train_df_base.with_columns(
    pl.col("so_nw_ct").shift(6).alias("m-3"),
    pl.col("so_nw_ct").shift(7).alias("m-4"),
    pl.col("so_nw_ct").shift(8).alias("m-5"),
).with_columns(
    ma6=(pl.col("m-0")+pl.col("m-1")+pl.col("m-2")+pl.col("m-3")+pl.col("m-4")+pl.col("m-5"))/6
)

# Feature set 3: add simple seasonal + trend (month sin/cos, period_index)
# We'll parse periods "YYYY MM" to a date and extract month and index

train_feat_season_trend = train_df_base.with_columns(
    period_dt = pl.col("periods").str.strptime(pl.Date, "%Y %m")
)

train_feat_season_trend = train_feat_season_trend.with_columns(
    month = pl.col("period_dt").dt.month().cast(pl.Int32),
    year = pl.col("period_dt").dt.year().cast(pl.Int32)
)

# numeric trend index: year * 12 + month
train_feat_season_trend = train_feat_season_trend.with_columns(
    period_index = (pl.col("year") * 12 + pl.col("month")).cast(pl.Int64)
)

# seasonal encoding (Polars-native trig)
train_feat_season_trend = train_feat_season_trend.with_columns(
    month_sin = (2 * np.pi * pl.col("month") / 12).sin(),
    month_cos = (2 * np.pi * pl.col("month") / 12).cos()
)






# Feature set 4: combined (lags + ma6 + season + trend)
train_feat_all = train_feat_lags_ma6.join(
    train_feat_season_trend.select(["key","periods","month_sin","month_cos","period_index"]),
    on=["key","periods"], how="left"
)

# Map of feature name -> dataframe
FEATURE_DFS = {
    "baseline": train_feat_baseline,
    "lags_ma6": train_feat_lags_ma6,
    "season_trend": train_feat_season_trend,
    "all": train_feat_all
}




# ===== CONFIGURE THIS CELL BEFORE RUN =====
FEATURE_NAME = "all"   # change to "baseline" / "lags_ma6" / "season_trend" / "all"
MODEL_LABEL = "LR"
start_test_period = datetime(2025, 7, 1)
test_period_count = 6
test_period_list = [ start_test_period + relativedelta(months=i) for i in range(test_period_count) ]
n_cpus = 18

# ===== DO NOT EDIT BELOW (unless you know what you do) =====
train_df_pol = FEATURE_DFS[FEATURE_NAME]

# build jobs (same pattern as you had)
jobs = []
for zone in tqdm(np.sort(train_df_pol["zone"].unique())):
    for test_period in test_period_list:
        curr_train_df = train_df_pol.filter(pl.col("zone") == zone).drop("zone").to_pandas()
        jobs.append((curr_train_df, test_period))

# run forecasting in parallel using your LR function
rs_parts = []
with Pool(n_cpus) as pool:
    for res in pool.imap_unordered(run_zone_forecasting_lr, jobs, chunksize=1):
        rs_parts.append(res)
rs_df = pd.concat(rs_parts, ignore_index=True)

# ===== EVALUATION (your code re-used) =====
rs_with_error_df = pl.from_pandas(rs_df).with_columns(
  re=pl.col("pred")-pl.col("so_nw_ct"),
  ae=(pl.col("pred")-pl.col("so_nw_ct")).abs()
).with_columns(
  mape=pl.when(pl.col("so_nw_ct") > 0)
      .then(pl.col("ae")/pl.col("so_nw_ct"))
      .otherwise(1)
).with_columns(
  mape=pl.when(pl.col("mape") > 1)
      .then(1)
      .otherwise(pl.col("mape"))
)

perform_df = rs_with_error_df.pivot(
  on="periods",
  index="key",
  values="mape",
  aggregate_function="sum",
  sort_columns=True
)

perform_df = perform_df.with_columns(
  mape_avg=perform_df.drop("key").mean_horizontal(),
  pareto80_flag=pl.when(pl.col("key").is_in(pareto_df["key"])).then(1).otherwise(0),
  zone=pl.col("key").str.split("_").list.get(0)
)

# Save snapshots to disk (model + feature in filename)
rs_fname = os.path.join(RESULTS_DIR, f"rs_with_error_{MODEL_LABEL}_{FEATURE_NAME}.csv")
perf_fname = os.path.join(RESULTS_DIR, f"perform_{MODEL_LABEL}_{FEATURE_NAME}.csv")
rs_with_error_df.write_csv(rs_fname)
perform_df.write_csv(perf_fname)
print("Saved:", rs_fname, perf_fname)

# Produce summary row for report
summary = perform_df.groupby("pareto80_flag").agg(pl.col("mape_avg").mean().round(4).alias("avg_mape"), pl.count().alias("comb_count"))
summary = summary.with_columns(
    pl.lit(MODEL_LABEL).alias("model"),
    pl.lit(FEATURE_NAME).alias("feature")
)
# convert to pandas for easier appending to a report list
summary_pd = summary.to_pandas()
display(summary_pd)

# Also keep the Polars frames in memory under model-feature names (optional)
globals()[f"rs_with_error_{MODEL_LABEL}_{FEATURE_NAME}"] = rs_with_error_df
globals()[f"perform_{MODEL_LABEL}_{FEATURE_NAME}"] = perform_df




# read all saved perform files and concat
all_files = [f for f in os.listdir(RESULTS_DIR) if f.startswith("perform_") and f.endswith(".csv")]
report_rows = []
for f in sorted(all_files):
    df = pl.read_csv(os.path.join(RESULTS_DIR, f))
    # ensure columns exist
    if "mape_avg" not in df.columns:
        # compute if needed (older files)
        df = df.with_columns(mape_avg = df.drop("key").mean_horizontal())
    # model and feature are encoded in filename: perform_<MODEL>_<FEATURE>.csv
    basename = f.replace("perform_","").replace(".csv","")
    model_name, feature_name = basename.split("_",1)
    df2 = df.select(["pareto80_flag","mape_avg"]).with_columns(
        pl.lit(model_name).alias("model"),
        pl.lit(feature_name).alias("feature")
    )
    report_rows.append(df2)

report_all = pl.concat(report_rows).groupby(["model","feature","pareto80_flag"]).agg(
    pl.col("mape_avg").mean().round(4).alias("avg_mape"),
    pl.count().alias("comb_count")
).sort(["model","feature","pareto80_flag"])
report_all.write_csv(os.path.join(RESULTS_DIR, "final_model_feature_report.csv"))
display(report_all)
print("Saved final report to:", os.path.join(RESULTS_DIR, "final_model_feature_report.csv"))



def run_zone_forecasting_lr_temp_debug(args):
    train_df, test_period = args

    rs_df = []

    max_train_period = (test_period - relativedelta(months=3)).strftime("%Y %m")
    test_period = test_period.strftime("%Y %m")

    curr_train_df = train_df.loc[train_df["periods"] <= max_train_period]
    curr_test_df = train_df.loc[train_df["periods"] == test_period]

    for key in np.sort(train_df["key"].unique()):

        curr_train_df_ = curr_train_df.loc[
            (curr_train_df["key"] == key) &
            (curr_train_df["periods"] <= max_train_period)
        ].drop(["key", "periods"], axis=1)

        if len(curr_train_df_) > 0:

            curr_test_df_ = curr_test_df.loc[
                (curr_test_df["key"] == key) &
                (curr_test_df["periods"] == test_period)
            ].drop(["key", "periods"], axis=1)

            # =========================
            # ðŸ” NaN DEBUG BLOCK
            # =========================
            if curr_test_df_.isna().any().any():
                print("\nðŸš¨ NaN FOUND")
                print("key:", key)
                print("period:", test_period)
                print("NaN count per column:")
                print(curr_test_df_.isna().sum())
                print("Row content:")
                print(curr_test_df_)
                print("=" * 60)

            # guard against empty test rows
            if len(curr_test_df_) == 0:
                continue

            if curr_test_df_.iloc[0]["ma3"] > 0:

                X_train = curr_train_df_.drop("so_nw_ct", axis=1)
                y_train = curr_train_df_["so_nw_ct"]

                X_test = curr_test_df_.drop("so_nw_ct", axis=1)
                y_test = curr_test_df_["so_nw_ct"]

                # model
                model = LinearRegression()
                model.fit(X_train, y_train)
                pred = model.predict(X_test)

                rs_df.append(pd.DataFrame({
                    "key": key,
                    "periods": test_period,
                    "m-0": X_test["m-0"].values,
                    "m-1": X_test["m-1"].values,
                    "m-2": X_test["m-2"].values,
                    "ma3": X_test["ma3"].values,
                    "so_nw_ct": y_test.values,
                    "pred": pred
                }))

            else:
                rs_df.append(pd.DataFrame({
                    "key": [key],
                    "periods": [test_period],
                    "m-0": [0],
                    "m-1": [0],
                    "m-2": [0],
                    "ma3": [0],
                    "so_nw_ct": [0],
                    "pred": [0]
                }))

        else:
            rs_df.append(pd.DataFrame({
                "key": [key],
                "periods": [test_period],
                "m-0": [0],
                "m-1": [0],
                "m-2": [0],
                "ma3": [0],
                "so_nw_ct": [0],
                "pred": [0]
            }))

    rs_df = pd.concat(rs_df, ignore_index=True)
    return rs_df




import polars as pl

feature_list = ["baseline", "lags_ma6", "season_trend", "all"]
results_summary = []

for feat in feature_list:
    df = pl.read_csv(f"results/perform_LR_{feat}.csv")
    avg_mape = df["mape_avg"].mean()
    results_summary.append({"feature": feat, "avg_mape": avg_mape})

summary_df = pl.DataFrame(results_summary)
print(summary_df.sort("avg_mape"))


best_feature = "all"

forecast_df = pl.read_csv(f"results/rs_with_error_LR_{best_feature}.csv")


# Show first 10 rows
print(forecast_df.head(10))

# Filter a zone
print(forecast_df.filter(pl.col("key").str.starts_with("ZONE02B")).head(10))


forecast_df.write_csv(f"results/forecast_LR_{best_feature}.csv")
print("Forecast saved:", f"results/forecast_LR_{best_feature}.csv")





forecast_zone2b = forecast_df.filter(pl.col("zone") == "ZONE02B")

forecast_zone2b_path = f"../data/forecast_LR_{best_feature}_ZONE02B.csv"
forecast_zone2b.write_csv(forecast_zone2b_path)

print("ZONE02B forecast saved:", forecast_zone2b_path)
